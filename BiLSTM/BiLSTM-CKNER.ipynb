{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e436b81d-5690-4f42-9a85-a773dcf60e76",
   "metadata": {},
   "source": [
    "# 1st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5964a28f-6df7-4453-88cc-852c03b6c97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "193/193 [==============================] - 12s 48ms/step - loss: 0.6048 - accuracy: 0.8803 - val_loss: 0.3651 - val_accuracy: 0.8925\n",
      "Epoch 2/10\n",
      "193/193 [==============================] - 9s 46ms/step - loss: 0.3026 - accuracy: 0.9073 - val_loss: 0.2337 - val_accuracy: 0.9261\n",
      "Epoch 3/10\n",
      "193/193 [==============================] - 9s 47ms/step - loss: 0.1854 - accuracy: 0.9432 - val_loss: 0.1301 - val_accuracy: 0.9625\n",
      "Epoch 4/10\n",
      "193/193 [==============================] - 9s 47ms/step - loss: 0.0940 - accuracy: 0.9729 - val_loss: 0.0764 - val_accuracy: 0.9790\n",
      "Epoch 5/10\n",
      "193/193 [==============================] - 9s 49ms/step - loss: 0.0546 - accuracy: 0.9844 - val_loss: 0.0588 - val_accuracy: 0.9847\n",
      "Epoch 6/10\n",
      "193/193 [==============================] - 11s 55ms/step - loss: 0.0373 - accuracy: 0.9891 - val_loss: 0.0509 - val_accuracy: 0.9863\n",
      "Epoch 7/10\n",
      "193/193 [==============================] - 11s 55ms/step - loss: 0.0280 - accuracy: 0.9918 - val_loss: 0.0467 - val_accuracy: 0.9882\n",
      "Epoch 8/10\n",
      "193/193 [==============================] - 11s 56ms/step - loss: 0.0220 - accuracy: 0.9936 - val_loss: 0.0440 - val_accuracy: 0.9894\n",
      "Epoch 9/10\n",
      "193/193 [==============================] - 11s 55ms/step - loss: 0.0181 - accuracy: 0.9947 - val_loss: 0.0434 - val_accuracy: 0.9908\n",
      "Epoch 10/10\n",
      "193/193 [==============================] - 11s 55ms/step - loss: 0.0152 - accuracy: 0.9955 - val_loss: 0.0472 - val_accuracy: 0.9907\n",
      "24/24 [==============================] - 1s 20ms/step - loss: 0.0529 - accuracy: 0.9893\n",
      "Test Loss: 0.052900832146406174\n",
      "Test Accuracy: 0.9892509579658508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\aga_ckner\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Function to read the dataset\n",
    "def read_data(file_path):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "            else:\n",
    "                word, tag = line.strip().split()\n",
    "                sentence.append((word, tag))\n",
    "        if sentence:  # Add the last sentence if there wasn't a newline at the end of the file\n",
    "            sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "# Read the dataset\n",
    "sentences = read_data('wlina_bd.txt')\n",
    "\n",
    "# Extract words and tags\n",
    "words = list(set([w[0] for s in sentences for w in s]))\n",
    "tags = list(set([w[1] for s in sentences for w in s]))\n",
    "\n",
    "# Add padding to words and tags\n",
    "words.append(\"ENDPAD\")\n",
    "n_words = len(words)\n",
    "n_tags = len(tags)\n",
    "\n",
    "# Encode the words and tags\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "\n",
    "# Prepare the data for the model\n",
    "max_len = 50  # Adjust as needed\n",
    "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=n_words - 1)\n",
    "\n",
    "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Build the BiLSTM model without CRF\n",
    "input = Input(shape=(max_len,))\n",
    "model = Embedding(input_dim=n_words, output_dim=50, input_length=max_len)(input)\n",
    "model = Dropout(0.1)(model)\n",
    "model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "model = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)  # Use softmax for output\n",
    "\n",
    "model = Model(input, model)\n",
    "\n",
    "# Compile the model with sparse categorical crossentropy loss\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, np.expand_dims(y_train, -1), batch_size=32, epochs=10, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, np.expand_dims(y_test, -1))\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff98b9a-7ffc-4c8e-a7a7-1d7777b0d650",
   "metadata": {},
   "source": [
    "# Step 2: SAVE BiLSTM-CRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65971fad-2bd2-47bd-ba02-a653defe454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"bilstm_ner_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd01b5-d92e-4fce-903a-eb516cdcd129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f1d32f-6d4d-4a08-992c-588be8165c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bed5b95f-1b54-48c1-a8a4-29b4a30ec31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 file contents:\n",
      "model_weights\n",
      "optimizer_weights\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# Verify and inspect HDF5 (.h5) file\n",
    "with h5py.File('bilstm_ner_model.h5', 'r') as h5_file:\n",
    "    print(\"HDF5 file contents:\")\n",
    "    for key in h5_file.keys():\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d54a35e-e9b8-4276-a1a5-9ee7e06b7752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras model summary:\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding_11 (Embedding)    (None, 50, 50)            616750    \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 50, 50)            0         \n",
      "                                                                 \n",
      " bidirectional_11 (Bidirect  (None, 50, 200)           120800    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " time_distributed_11 (TimeD  (None, 50, 11)            2211      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 739761 (2.82 MB)\n",
      "Trainable params: 739761 (2.82 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Verify and inspect Keras (.keras) file\n",
    "model_keras = load_model('bilstm_ner_model.keras')\n",
    "print(\"Keras model summary:\")\n",
    "model_keras.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1dddfd-d479-436c-8dec-a11915df3199",
   "metadata": {},
   "source": [
    "# Test keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56496949-da11-4446-859b-95fcb9c6427c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 251ms/step\n",
      "مەهدی: B-PER\n",
      "ئۆزدەمیر: I-PER\n",
      "رایگەیاند: O\n",
      "لە: O\n",
      "هەرێمی: B-LOC\n",
      "کوردستان: I-LOC\n",
      "و: O\n",
      "بەریتانیا: B-LOC\n",
      "و: O\n",
      "ئێران: B-LOC\n",
      "و: O\n",
      "تورکیا: B-LOC\n",
      "پاشان: O\n",
      "پارتی: B-ORG\n",
      "دیموکراتی: I-ORG\n",
      "کوردستان: I-ORG\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the model\n",
    "model = load_model('bilstm_ner_model.keras')\n",
    "\n",
    "# Kurdish Sorani text example (replace this with your actual text)\n",
    "kurdish_text = \"مەهدی ئۆزدەمیر رایگەیاند لە هەرێمی کوردستان و بەریتانیا و ئێران و تورکیا پاشان پارتی دیموکراتی کوردستان\"\n",
    "\n",
    "# Preprocess the input text\n",
    "words = kurdish_text.split()  # Simple split by space\n",
    "word_indices = [word2idx.get(word, word2idx[\"ENDPAD\"]) for word in words]  # Convert words to indices\n",
    "X_test = pad_sequences([word_indices], maxlen=50, padding=\"post\", value=n_words - 1)  # Pad sequence\n",
    "\n",
    "# Predict the tags\n",
    "y_pred = model.predict(X_test)\n",
    "predicted_tags = np.argmax(y_pred, axis=-1)\n",
    "\n",
    "# Convert indices to tags\n",
    "predicted_tags = [list(tag2idx.keys())[list(tag2idx.values()).index(tag)] for tag in predicted_tags[0]]\n",
    "\n",
    "# Print the results\n",
    "for word, tag in zip(words, predicted_tags):\n",
    "    print(f\"{word}: {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c785effb-3806-44f7-abe3-da8785d92425",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
